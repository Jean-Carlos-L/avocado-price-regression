# Normalizaci√≥n y Limpieza de Datos del Dataset de Aguacates ü•ë

## 1Ô∏è‚É£ Revisi√≥n y Limpieza Inicial

El dataset fue revisado utilizando `df.info()` y `df.describe()` para detectar valores nulos, tipos de datos y rangos de valores. No se encontraron valores nulos ni duplicados significativos, lo cual permiti√≥ continuar con la estandarizaci√≥n de las columnas.

Posteriormente, se realizaron los siguientes pasos de limpieza:

- Eliminaci√≥n de la columna `Unnamed: 0`, ya que solo representaba un √≠ndice sin valor anal√≠tico.
- Conversi√≥n de la columna `date` a formato de fecha (`datetime`).
- Creaci√≥n de columnas adicionales derivadas: `date_ts` (timestamp num√©rico) y `month` (mes del a√±o).
- Renombramiento de las columnas con c√≥digos PLU (`4046`, `4225`, `4770`) por nombres descriptivos:  
  `small_hass_sold`, `large_hass_sold`, `xlarge_hass_sold`.

---

## 2Ô∏è‚É£ Codificaci√≥n de Variables Categ√≥ricas

Se aplic√≥ **codificaci√≥n dummy** √∫nicamente a la columna `type`, ya que la variable `region` contiene demasiadas categor√≠as, lo que aumentar√≠a innecesariamente la dimensionalidad del dataset.

```python
df = pd.get_dummies(df, columns=['type'], drop_first=True)
```

Esto gener√≥ una nueva columna `type_organic`, que toma valores booleanos (`True`/`False`), indicando si el aguacate es org√°nico.

---

## 3Ô∏è‚É£ Selecci√≥n de Variables Num√©ricas

Se identificaron las variables num√©ricas que requer√≠an ser llevadas a una misma escala antes de aplicar modelos predictivos. Estas variables est√°n asociadas a precios y vol√∫menes de ventas, por lo que sus rangos difieren significativamente:

```python
cols_to_scale = [
    'averageprice', 'total_volume', 'small_hass_sold',
    'large_hass_sold', 'xlarge_hass_sold', 'total_bags',
    'small_bags', 'large_bags', 'xlarge_bags'
]
```

Mientras que `averageprice` tiene valores cercanos a 1 o 2 d√≥lares, las variables de volumen como `total_volume` o `small_hass_sold` pueden alcanzar valores de millones de unidades.  
Dejar estas diferencias de escala sin tratar podr√≠a causar **sesgos en el modelo**, ya que las variables con valores m√°s grandes dominar√≠an las distancias o gradientes durante el entrenamiento.

---

## 4Ô∏è‚É£ Aplicaci√≥n del Escalado Estandarizado (Z-Score)

Para la normalizaci√≥n se utiliz√≥ el m√©todo **Z-Score**, implementado con `StandardScaler` de `scikit-learn`.  
Este m√©todo transforma los datos de forma que cada variable tenga una **media de 0** y una **desviaci√≥n est√°ndar de 1**, seg√∫n la f√≥rmula:

\(
Z = rac{X - \mu}{\sigma}
\)

Donde:
- \( X \) es el valor original,
- \( \mu \) es la media de la variable,
- \( \sigma \) es la desviaci√≥n est√°ndar.

C√≥digo aplicado:

```python
from sklearn.preprocessing import StandardScaler

scaler = StandardScaler()
df_scaled = df.copy()
df_scaled[cols_to_scale] = scaler.fit_transform(df[cols_to_scale])
```

---

## 5Ô∏è‚É£ Verificaci√≥n del Resultado

Despu√©s de aplicar el escalado, se revis√≥ la estructura del nuevo DataFrame (`df_scaled`) para confirmar que todas las columnas fueron transformadas correctamente y que no se alteraron los tipos de datos no num√©ricos:

```python
df_scaled.info()
```

Resultado principal:
- Las columnas num√©ricas presentan valores reescalados (media ‚âà 0, desviaci√≥n est√°ndar ‚âà 1).  
- El resto de las variables (`date`, `region`, `type_organic`, `month`, `year`) conservaron su estructura original.  
- No se generaron valores nulos o inconsistentes durante el proceso.

Esto se valid√≥ con el comando:

```python
df_scaled.describe()
```

Los resultados mostraron que las variables num√©ricas tienen:
- Media muy cercana a **0**.  
- Desviaci√≥n est√°ndar muy cercana a **1**.  
Esto confirma que la normalizaci√≥n se aplic√≥ correctamente.

---

## 6Ô∏è‚É£ Justificaci√≥n del M√©todo Utilizado

La elecci√≥n de **Z-Score (StandardScaler)** se justifica por las siguientes razones:

1. **Adecuaci√≥n para modelos lineales y de distancia**  
   El escalado estandarizado es ideal para algoritmos como regresi√≥n lineal, regresi√≥n log√≠stica, SVM o k-means, que son sensibles a la escala de las variables.

2. **Conserva la distribuci√≥n original**  
   A diferencia de una normalizaci√≥n min-max, el m√©todo Z-score no comprime los valores dentro de un rango fijo (como 0‚Äì1), sino que mantiene la forma de la distribuci√≥n, lo cual es √∫til si existen colas largas o datos dispersos.

3. **Interpretabilidad**  
   Los valores transformados pueden interpretarse en t√©rminos de desviaciones est√°ndar respecto a la media, facilitando la detecci√≥n posterior de valores at√≠picos o extremos.

---

## 7Ô∏è‚É£ Conclusiones del Proceso de Normalizaci√≥n

- Se logr√≥ **estandarizar las variables num√©ricas** garantizando comparabilidad entre ellas.  
- Las variables categ√≥ricas se manejaron con codificaci√≥n *dummy*, pero √∫nicamente para la columna `type` (dando origen a `type_organic`), evitando un exceso de variables derivadas de `region`.  
- El conjunto de datos resultante (`df_scaled`) se encuentra limpio, sin duplicados ni valores nulos, y preparado para el **entrenamiento de modelos predictivos de precios**.

En s√≠ntesis, el proceso de normalizaci√≥n permiti√≥ homogeneizar la escala de los datos, mejorar la calidad estad√≠stica del conjunto y sentar las bases para un modelado m√°s preciso y estable.
