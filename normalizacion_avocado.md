# Normalizaci√≥n y Limpieza de Datos ‚Äî Avocado Prices ü•ë

**Archivo:** `normalizacion_avocado.md`

**Resumen breve**
Este documento describe el flujo completo realizado para limpiar, detectar y tratar outliers, transformar fechas y normalizar (Z-Score) el dataset _Avocado Prices_. Incluye explicaciones conceptuales, fragmentos de c√≥digo usados en el notebook y observaciones sobre los resultados.

## 1Ô∏è‚É£ Revisi√≥n y limpieza inicial
Se revis√≥ la estructura con `df.info()` y `df.describe()` para detectar nulos y tipos de datos. No se encontraron valores nulos significativos.

Pasos aplicados:
- Eliminaci√≥n de la columna `Unnamed: 0` (√≠ndice redundante).
- Conversi√≥n de `Date` a `datetime`.
- Creaci√≥n de `days_since_start` (d√≠as desde la primera observaci√≥n) para usar la fecha como variable num√©rica.
- Extracci√≥n del `month` para an√°lisis estacional.
- Renombramiento de las columnas PLU (4046, 4225, 4770) a nombres descriptivos para mayor legibilidad.

```python
# Limpieza y columnas derivadas
df.drop(columns=['Unnamed: 0'], inplace=True)
df['Date'] = pd.to_datetime(df['Date'])
df['days_since_start'] = (df['Date'] - df['Date'].min()).dt.days
df['month'] = df['Date'].dt.month

# Renombrar PLU y columnas con espacios
df.rename(columns={
    '4046': 'small_hass_sold',
    '4225': 'large_hass_sold',
    '4770': 'xlarge_hass_sold',
    'AveragePrice': 'averageprice',
    'Total Volume': 'total_volume',
    'Total Bags': 'total_bags',
    'Small Bags': 'small_bags',
    'Large Bags': 'large_bags',
    'XLarge Bags': 'xlarge_bags'
}, inplace=True)
```

**Comentario:** se mantuvieron `total_bags` y el desglose por tipo de bolsa porque representan **empaque** (unidad distinta a PLU). Las columnas `*_hass_sold` reflejan conteos por PLU (unidades vendidas por tama√±o).

---

## 2Ô∏è‚É£ Detecci√≥n y tratamiento de datos at√≠picos (outliers)
### 2.1 ¬øPor qu√© tratarlos?
Los outliers pueden distorsionar medias, varianzas y afectar el ajuste de modelos. En este dataset, las variables de volumen (`total_volume`, `*_hass_sold`, `*_bags`) muestran colas largas, por lo que es necesario detectarlos y decidir una estrategia.

### 2.2 M√©todo seleccionado: IQR (Interquartile Range)
Se eligi√≥ IQR por su robustez frente a distribuciones asim√©tricas. Se definieron l√≠mites por variable:

\( lower = Q1 - 1.5 \times IQR \)
\( upper = Q3 + 1.5 \times IQR \)

**Estrategia aplicada:** en lugar de eliminar filas, **se caparon** (winsorize) los valores fuera de los l√≠mites substituy√©ndolos por el l√≠mite inferior o superior correspondiente. Esto preserva el tama√±o de muestra y evita perder informaci√≥n √∫til.

```python
numeric_cols = [
    'averageprice', 'total_volume', 'small_hass_sold', 'large_hass_sold',
    'xlarge_hass_sold', 'total_bags', 'small_bags', 'large_bags', 'xlarge_bags'
]

for col in numeric_cols:
    Q1 = df[col].quantile(0.25)
    Q3 = df[col].quantile(0.75)
    IQR = Q3 - Q1
    lower = Q1 - 1.5 * IQR
    upper = Q3 + 1.5 * IQR
    df[col] = np.where(df[col] < lower, lower,
              np.where(df[col] > upper, upper, df[col]))
```

### 2.3 Visualizaci√≥n antes/despu√©s
- Boxplots generales para todas las variables num√©ricas (orientados horizontalmente) para detectar outliers globales.

![Outliers BoxPlot](./img/outliersboxplot.png)

- Boxplot de `averageprice` por `type` (conventional vs organic).

![Outliers del promedio del precio por tipo](./img/boxplotOutLiersvsType.png)

- Scatter `total_volume` vs `averageprice` (eje X en escala log) para ver c√≥mo se comportan los outliers respecto al volumen.

!(./img/scatterplotPrecioVTotal.png)

**Observaci√≥n visual:** exist√≠an outliers en vol√∫menes grandes y precios extremos; despu√©s del capado con IQR, las colas quedaron menos pronunciadas.

---

## 3Ô∏è‚É£ An√°lisis cuantitativo de outliers
Para `averageprice` se calcul√≥ el conjunto de observaciones fuera de los l√≠mites IQR.



```python
# C√°lculo de l√≠mites con IQR, IQR es el rango intercuart√≠lico
Q1 = df['averageprice'].quantile(0.25)
Q3 = df['averageprice'].quantile(0.75)
IQR = Q3 - Q1
limite_inferior = Q1 - 1.5 * IQR
limite_superior = Q3 + 1.5 * IQR

# Separar datos normales y outliers
outliers = df[(df['averageprice'] < limite_inferior) | (df['averageprice'] > limite_superior)]
normales = df[(df['averageprice'] >= limite_inferior) & (df['averageprice'] <= limite_superior)]

#miramos los tama√±os de las muestras para verificar la cantidad de outliers

outliers.shape, normales.shape

#como resultado tenemos 
((0, 15), (18249, 15))
```

- Comparaci√≥n del `averageprice` con y sin outliers
![](./img/comparacion.png)

**Resultado observado:** el precio promedio con y sin outliers fue aproximadamente `1.40`, lo que indica que los outliers no alteran de forma significativa el promedio del dataset global (probablemente por su baja proporci√≥n o por distribuci√≥n balanceada de extremos altos y bajos).

---

## 4Ô∏è‚É£ Codificaci√≥n de variables categ√≥ricas
Se aplic√≥ codificaci√≥n dummy solo a `type` para evitar explotar la dimensionalidad por `region`:

```python
df = pd.get_dummies(df, columns=['type'], drop_first=True)
# Genera la columna `type_organic` (True si es organic)
```

**Motivo:** `region` tiene muchas categor√≠as (alto cardinality), por lo que convertirla en dummies aumentar√≠a mucho las columnas y complicar√≠a modelos sencillos. Si fuera necesario, se pueden usar t√©cnicas alternativas (target encoding, embeddings, clustering de regiones, etc.).

---

## 5Ô∏è‚É£ Escalado estandarizado (Z-Score)
Se identificaron las columnas num√©ricas a escalar y se aplic√≥ `StandardScaler`:

```python
cols_to_scale = [
    'averageprice', 'total_volume', 'small_hass_sold', 'large_hass_sold',
    'xlarge_hass_sold', 'total_bags', 'small_bags', 'large_bags', 'xlarge_bags'
]

scaler = StandardScaler()
df_scaled = df.copy()
df_scaled[cols_to_scale] = scaler.fit_transform(df[cols_to_scale])
```

**Verificaci√≥n:**
```python
print(df_scaled.info())
print(df_scaled.describe().T.head())

#como resultado tenemos

<class 'pandas.core.frame.DataFrame'>
RangeIndex: 18249 entries, 0 to 18248
Data columns (total 15 columns):
 #   Column            Non-Null Count  Dtype         
---  ------            --------------  -----         
 0   Date              18249 non-null  datetime64[ns]
 1   averageprice      18249 non-null  float64       
 2   total_volume      18249 non-null  float64       
 3   small_hass_sold   18249 non-null  float64       
 4   large_hass_sold   18249 non-null  float64       
 5   xlarge_hass_sold  18249 non-null  float64       
 6   total_bags        18249 non-null  float64       
 7   small_bags        18249 non-null  float64       
 8   large_bags        18249 non-null  float64       
 9   xlarge_bags       18249 non-null  float64       
 10  year              18249 non-null  int64         
 11  region            18249 non-null  object        
 12  days_since_start  18249 non-null  int64         
 13  month             18249 non-null  int32         
 14  type_organic      18249 non-null  bool          
dtypes: bool(1), datetime64[ns](1), float64(9), int32(1), int64(2), object(1)
memory usage: 1.9+ MB
None
                   count                           mean                  min  \
Date               18249  2016-08-13 23:30:43.498273792  2015-01-04 00:00:00   
...
averageprice                0.648259             2.772605  1.000027  
total_volume                0.388931              2.09939  1.000027  
small_hass_sold             0.418119             2.083229  1.000027  
large_hass_sold             0.386784             2.045275  1.000027  
Output is truncated. View as a scrollable element or open in a text editor. Adjust cell output settings...
```

Salida ejemplo (resumen):
- `df_scaled` contiene 18249 filas y 15 columnas.
- Las columnas num√©ricas escaladas tienen media cercana a 0 y desviaci√≥n est√°ndar cercana a 1.

---

